{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFyyMECEdvGPnjoZuB1y2t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","class DenseLayer:\n","    \"\"\"\n","    Clase que representa una capa densa (fully connected) en una red neuronal.\n","\n","    Atributos:\n","    - num_neurons: Número de neuronas en la capa.\n","    - activation: Función de activación a utilizar (ej. tf.nn.relu).\n","    - weights: Tensor que representa los pesos de la capa.\n","    - biases: Tensor que representa los sesgos de la capa.\n","    - batch_size: Tamaño del batch para el entrenamiento.\n","    - prev_num_neurons: Número de neuronas en la capa anterior.\n","    \"\"\"\n","\n","    def __init__(self, num_neurons=1, activation=None, batch_size=1, prev_num_neurons=1):\n","        \"\"\"\n","        Constructor de la clase DenseLayer.\n","\n","        Parámetros:\n","        - num_neurons: Número de neuronas en la capa.\n","        - activation: Función de activación a aplicar.\n","        - batch_size: Tamaño del lote (batch size).\n","        - prev_num_neurons: Número de neuronas en la capa anterior.\n","        \"\"\"\n","        self.num_neurons = num_neurons\n","        self.activation = activation\n","        # Inicialización de pesos con distribución normal, escalada por la raíz cuadrada del número de neuronas previas\n","        self.weights = tf.Variable(tf.random.normal(shape=[prev_num_neurons, num_neurons], stddev=tf.sqrt(2.0 / prev_num_neurons)), trainable=True)\n","        # Inicialización de sesgos con una media de 0.1 y una desviación estándar de 0.01\n","        self.biases = tf.Variable(tf.random.normal(shape=[1, num_neurons], mean=0.1, stddev=0.01), trainable=True)\n","        self.batch_size = batch_size\n","        self.prev_num_neurons = prev_num_neurons\n","\n","    def update_weights_and_biases(self, new_weights, new_biases):\n","        \"\"\"\n","        Método para actualizar los pesos y sesgos de la capa.\n","\n","        Parámetros:\n","        - new_weights: Nuevos valores para los pesos.\n","        - new_biases: Nuevos valores para los sesgos.\n","        \"\"\"\n","        self.weights.assign(new_weights)\n","        self.biases.assign(new_biases)\n","\n","    def run(self, input):\n","        \"\"\"\n","        Método que realiza la pasada hacia adelante (forward pass) en la capa.\n","\n","        Parámetros:\n","        - input: Entrada a la capa (puede ser la salida de una capa anterior).\n","\n","        Retorna:\n","        - output: Salida de la capa después de aplicar la función de activación.\n","        \"\"\"\n","        # Calcular el output como el producto matricial de la entrada y los pesos, sumado a los sesgos\n","        output = tf.matmul(input, self.weights) + self.biases\n","        # Aplicar la función de activación si está definida\n","        if self.activation is not None:\n","            output = self.activation(output)\n","        return output\n","\n","class FeedForwardModel:\n","    \"\"\"\n","    Clase que representa un modelo feedforward de red neuronal.\n","\n","    Atributos:\n","    - layers: Lista de capas densas (DenseLayer).\n","    - inputs: Entrada del modelo.\n","    - outputs: Salida esperada (etiquetas).\n","    - loss: Función de pérdida a utilizar (ej. tf.losses.MeanSquaredError).\n","    - learning_rate: Tasa de aprendizaje para el optimizador.\n","    - value_loss: Último valor de la pérdida calculada.\n","    - value_accuracy: Último valor de la precisión calculada.\n","    - r2: Último valor del coeficiente de determinación R^2.\n","    \"\"\"\n","\n","    def __init__(self, layers, inputs, outputs, loss, learning_rate):\n","        \"\"\"\n","        Constructor de la clase FeedForwardModel.\n","\n","        Parámetros:\n","        - layers: Lista de capas que componen el modelo.\n","        - inputs: Tensor que representa la entrada al modelo.\n","        - outputs: Tensor que representa la salida esperada.\n","        - loss: Función de pérdida a utilizar.\n","        - learning_rate: Tasa de aprendizaje.\n","        \"\"\"\n","        self.layers = layers\n","        self.inputs = inputs\n","        self.outputs = outputs\n","        self.loss = loss\n","        self.learning_rate = learning_rate\n","        self.value_loss = 0\n","        self.value_accuracy = 0\n","        self.r2 = 0\n","\n","    def forward_pass(self, input):\n","        \"\"\"\n","        Realiza la pasada hacia adelante (forward pass) a través de todas las capas del modelo.\n","\n","        Parámetros:\n","        - input: Entrada al modelo.\n","\n","        Retorna:\n","        - input: Salida después de pasar por todas las capas.\n","        \"\"\"\n","        for layer in self.layers:\n","            input = layer.run(input)\n","        self.logits = input\n","        return input\n","\n","    def step(self):\n","        \"\"\"\n","        Realiza un paso de entrenamiento que incluye el cálculo del gradiente y la actualización de los pesos.\n","\n","        Retorna:\n","        - loss: Valor de la pérdida calculada.\n","        - logits: Predicciones del modelo.\n","        \"\"\"\n","        with tf.GradientTape() as tape:\n","            # Realizar la pasada hacia adelante y calcular la pérdida\n","            logits = self.forward_pass(self.inputs)\n","            loss = self.loss(self.outputs, logits)\n","\n","        # Obtener los pesos y sesgos de cada capa\n","        weights_and_biases = [(layer.weights, layer.biases) for layer in self.layers]\n","        # Calcular los gradientes con respecto a la pérdida\n","        gradients = tape.gradient(loss, [wb for pair in weights_and_biases for wb in pair])\n","\n","        # Actualizar los pesos y sesgos en cada capa\n","        for layer, grad_pair in zip(self.layers, zip(*[iter(gradients)]*2)):\n","            weights_grad, biases_grad = grad_pair\n","            new_weights = layer.weights - self.learning_rate * weights_grad\n","            new_biases = layer.biases - self.learning_rate * biases_grad\n","            layer.update_weights_and_biases(new_weights, new_biases)\n","\n","        return loss, logits\n","\n","    def train(self, epochs):\n","        \"\"\"\n","        Entrena el modelo durante un número especificado de épocas.\n","\n","        Parámetros:\n","        - epochs: Número de épocas para entrenar.\n","        \"\"\"\n","        for epoch in range(epochs):\n","            loss, logits = self.step()\n","            self.value_loss = loss.numpy()\n","            print(f\"------> Epoch {epoch+1}: Loss = {self.value_loss}\")\n","\n","    def predict(self, input):\n","        \"\"\"\n","        Realiza una predicción con el modelo entrenado.\n","\n","        Parámetros:\n","        - input: Entrada para la predicción.\n","\n","        Retorna:\n","        - Salida del modelo después de la pasada hacia adelante.\n","        \"\"\"\n","        return self.forward_pass(input)\n"],"metadata":{"id":"5_-L3y9cp8nn","executionInfo":{"status":"ok","timestamp":1723756986203,"user_tz":360,"elapsed":222,"user":{"displayName":"Joaquín Solórzano","userId":"18010194674409538512"}}},"execution_count":133,"outputs":[]},{"cell_type":"code","source":["layers = [DenseLayer(4, tf.nn.relu, prev_num_neurons=1), DenseLayer(3, tf.nn.relu, prev_num_neurons=4), DenseLayer(1, None, prev_num_neurons=3)]\n","\n","X = tf.constant([[1], [2], [3], [10]], dtype=tf.float32)\n","Y = tf.constant([[2], [4], [6], [20]], dtype=tf.float32)\n","\n","loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","model = FeedForwardModel(layers, X, Y, loss_fn, 0.001)\n","\n","model.train(100)\n","\n","prediction = model.predict(tf.constant([[10]], dtype=tf.float32))\n","print(\"\\n\\nPrediction:\", prediction.numpy())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BVBBjvLv6v4M","executionInfo":{"status":"ok","timestamp":1723757017131,"user_tz":360,"elapsed":1423,"user":{"displayName":"Joaquín Solórzano","userId":"18010194674409538512"}},"outputId":"445ee58e-a3ee-40f4-be29-ecbc052ccc1f"},"execution_count":134,"outputs":[{"output_type":"stream","name":"stdout","text":["------> Epoch 1: Loss = 98.9967041015625\n","------> Epoch 2: Loss = 93.49415588378906\n","------> Epoch 3: Loss = 87.12461853027344\n","------> Epoch 4: Loss = 79.71963500976562\n","------> Epoch 5: Loss = 71.06635284423828\n","------> Epoch 6: Loss = 61.14790725708008\n","------> Epoch 7: Loss = 50.13909912109375\n","------> Epoch 8: Loss = 38.56555938720703\n","------> Epoch 9: Loss = 27.31456756591797\n","------> Epoch 10: Loss = 17.479196548461914\n","------> Epoch 11: Loss = 9.964757919311523\n","------> Epoch 12: Loss = 5.063906669616699\n","------> Epoch 13: Loss = 2.3649747371673584\n","------> Epoch 14: Loss = 1.1001954078674316\n","------> Epoch 15: Loss = 0.5827106833457947\n","------> Epoch 16: Loss = 0.3907861113548279\n","------> Epoch 17: Loss = 0.32329148054122925\n","------> Epoch 18: Loss = 0.2993602752685547\n","------> Epoch 19: Loss = 0.28987565636634827\n","------> Epoch 20: Loss = 0.28503912687301636\n","------> Epoch 21: Loss = 0.281681090593338\n","------> Epoch 22: Loss = 0.27880173921585083\n","------> Epoch 23: Loss = 0.2760869562625885\n","------> Epoch 24: Loss = 0.27344027161598206\n","------> Epoch 25: Loss = 0.2708319127559662\n","------> Epoch 26: Loss = 0.26825276017189026\n","------> Epoch 27: Loss = 0.26569944620132446\n","------> Epoch 28: Loss = 0.2631716728210449\n","------> Epoch 29: Loss = 0.2606672942638397\n","------> Epoch 30: Loss = 0.258187472820282\n","------> Epoch 31: Loss = 0.25573068857192993\n","------> Epoch 32: Loss = 0.25329774618148804\n","------> Epoch 33: Loss = 0.2508888244628906\n","------> Epoch 34: Loss = 0.24850186705589294\n","------> Epoch 35: Loss = 0.2461378425359726\n","------> Epoch 36: Loss = 0.24379664659500122\n","------> Epoch 37: Loss = 0.2414768487215042\n","------> Epoch 38: Loss = 0.23918040096759796\n","------> Epoch 39: Loss = 0.23690520226955414\n","------> Epoch 40: Loss = 0.23465175926685333\n","------> Epoch 41: Loss = 0.2324196994304657\n","------> Epoch 42: Loss = 0.23020891845226288\n","------> Epoch 43: Loss = 0.2280200719833374\n","------> Epoch 44: Loss = 0.22585099935531616\n","------> Epoch 45: Loss = 0.22370275855064392\n","------> Epoch 46: Loss = 0.22157496213912964\n","------> Epoch 47: Loss = 0.21946753561496735\n","------> Epoch 48: Loss = 0.2173803299665451\n","------> Epoch 49: Loss = 0.21531230211257935\n","------> Epoch 50: Loss = 0.21326453983783722\n","------> Epoch 51: Loss = 0.21123582124710083\n","------> Epoch 52: Loss = 0.20922625064849854\n","------> Epoch 53: Loss = 0.20723608136177063\n","------> Epoch 54: Loss = 0.20526468753814697\n","------> Epoch 55: Loss = 0.20331254601478577\n","------> Epoch 56: Loss = 0.2013784795999527\n","------> Epoch 57: Loss = 0.19946259260177612\n","------> Epoch 58: Loss = 0.19756513833999634\n","------> Epoch 59: Loss = 0.19568537175655365\n","------> Epoch 60: Loss = 0.19382405281066895\n","------> Epoch 61: Loss = 0.1919797956943512\n","------> Epoch 62: Loss = 0.19015374779701233\n","------> Epoch 63: Loss = 0.18834389746189117\n","------> Epoch 64: Loss = 0.18655157089233398\n","------> Epoch 65: Loss = 0.18477684259414673\n","------> Epoch 66: Loss = 0.18301813304424286\n","------> Epoch 67: Loss = 0.18127673864364624\n","------> Epoch 68: Loss = 0.17955149710178375\n","------> Epoch 69: Loss = 0.17784275114536285\n","------> Epoch 70: Loss = 0.1761501133441925\n","------> Epoch 71: Loss = 0.17447292804718018\n","------> Epoch 72: Loss = 0.17281289398670197\n","------> Epoch 73: Loss = 0.17116762697696686\n","------> Epoch 74: Loss = 0.16953757405281067\n","------> Epoch 75: Loss = 0.1679236888885498\n","------> Epoch 76: Loss = 0.1663248986005783\n","------> Epoch 77: Loss = 0.16474135220050812\n","------> Epoch 78: Loss = 0.16317300498485565\n","------> Epoch 79: Loss = 0.16161870956420898\n","------> Epoch 80: Loss = 0.16007956862449646\n","------> Epoch 81: Loss = 0.15855492651462555\n","------> Epoch 82: Loss = 0.15704461932182312\n","------> Epoch 83: Loss = 0.1555488109588623\n","------> Epoch 84: Loss = 0.15406693518161774\n","------> Epoch 85: Loss = 0.1525992751121521\n","------> Epoch 86: Loss = 0.15114544332027435\n","------> Epoch 87: Loss = 0.14970539510250092\n","------> Epoch 88: Loss = 0.14827904105186462\n","------> Epoch 89: Loss = 0.14686568081378937\n","------> Epoch 90: Loss = 0.1454664021730423\n","------> Epoch 91: Loss = 0.14407935738563538\n","------> Epoch 92: Loss = 0.14270614087581635\n","------> Epoch 93: Loss = 0.14134536683559418\n","------> Epoch 94: Loss = 0.13999833166599274\n","------> Epoch 95: Loss = 0.1386631429195404\n","------> Epoch 96: Loss = 0.1373414248228073\n","------> Epoch 97: Loss = 0.13603158295154572\n","------> Epoch 98: Loss = 0.13473398983478546\n","------> Epoch 99: Loss = 0.13344910740852356\n","------> Epoch 100: Loss = 0.13217619061470032\n","\n","\n","Prediction: [[19.763271]]\n"]}]}]}